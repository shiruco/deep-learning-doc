{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習に関するテクニック\n",
    "#### 確率的勾配降下法\n",
    "勾配方向にパラメータを更新する方法。  \n",
    "stochastic gradient descent 略してSGD。  \n",
    "\n",
    "##### SGDの欠点\n",
    "関数の形状が等方的でない（伸びた形の関数だと）非効率な経路で探索してしまう。\n",
    "\n",
    "#### Momentum\n",
    "関連性のある方向へSGDを加速させ振動を抑制する方法。\n",
    "\n",
    "#### AdaGrad\n",
    "パラメータの要素ごとに適応的に学習係数を調整しながら学習を行う手法。  \n",
    "\n",
    "#### Adam\n",
    "MomentumとAdaGradを融合したような手法。  \n",
    "\n",
    "#### 重みの初期値\n",
    "重みの初期値は0にしてはいけない。  \n",
    "正確には重みを均一な値にしてはいけない。  \n",
    "=> 誤差逆伝播法において、すべての重みの値が均一に更新されてしまうから。  \n",
    "(乗算ノードの逆伝播を思い出す)\n",
    "\n",
    "#### Xavierの初期値\n",
    "前層のノード数をnとしたとき、標準偏差が１/√nの標準偏差を持つガウス分布  \n",
    "=> sigmoid関数やtanh関数に適している  \n",
    "\n",
    "#### Heの初期値\n",
    "前層のノード数をnとしたとき、標準偏差が１/√2/nの標準偏差を持つガウス分布  \n",
    "=> ReLU関数に適している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "各層でのアクティベーション分布を、適度な広がりを持つように調整する\n",
    "\n",
    "**特徴** \n",
    "- 学習を速く進行させることができる(学習係数を大きくすることができる)\n",
    "- 初期値にそれほど依存しない\n",
    "- 過学習を抑制する\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 過学習の問題\n",
    "**過学習とは**\n",
    "訓練データだけに適応しすぎてしまい、他のデータにうまく対応できない状態。  \n",
    "\n",
    "**原因**\n",
    "- パラメータを大量に持ち、表現力の高いモデルであること\n",
    "- 訓練データが少ないこと\n",
    "\n",
    "#### Weight decay\n",
    "学習の過程において、大きな重みを持つことに対してペナルティを課すことで、過学習を抑制する。  \n",
    "損失関数に対して重みのL2ノルムを加算する。  \n",
    "\n",
    "#### Dropout\n",
    "ニューラルネットワークのモデルが複雑になってくるとWeight decayだけでは対応が困難。  \n",
    "Dropoutとはニューロンをランダムに消去しながら学習する手法。  \n",
    "\n",
    "### ハイパーパラメータの最適化\n",
    "ポイントはハイパーパラメータの「良い値」が存在する範囲を徐々に絞り込んでいくこと。  \n",
    "(ベイズ最適化の定理を駆使)\n",
    "\n",
    "Step0  \n",
    "ハイパーパラメータの範囲を設定する。  \n",
    "\n",
    "Step1  \n",
    "設定されたハイパーパラメータの範囲から、ランダムにサンプリングする。  \n",
    "\n",
    "Step2  \n",
    "ステップ１でサンプリングした値を使用して学習を行い、検証データで認識精度を評価する。  \n",
    "\n",
    "Step3  \n",
    "Step1とStep2をある回数（100回くらい）繰り返し、それらの認識精度の結果から、ハイパーパラメータの範囲を狭める。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.util import shuffle_dataset\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 高速化のため訓練データの削減\n",
    "x_train = x_train[:500]\n",
    "t_train = t_train[:500]\n",
    "\n",
    "# 検証データの分離\n",
    "validation_rate = 0.20\n",
    "validation_num = x_train.shape[0] * validation_rate\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]\n",
    "\n",
    "\n",
    "def __train(lr, weight_decay, epocs=50):\n",
    "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                            output_size=10, weight_decay_lambda=weight_decay)\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                      epochs=epocs, mini_batch_size=100,\n",
    "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.test_acc_list, trainer.train_acc_list\n",
    "\n",
    "\n",
    "# ハイパーパラメータのランダム探索======================================\n",
    "optimization_trial = 100\n",
    "results_val = {}\n",
    "results_train = {}\n",
    "for _ in range(optimization_trial):\n",
    "    # 探索したハイパーパラメータの範囲を指定===============\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "    lr = 10 ** np.random.uniform(-6, -2)\n",
    "    # ================================================\n",
    "\n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list\n",
    "\n",
    "# グラフの描画========================================================\n",
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5: plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
